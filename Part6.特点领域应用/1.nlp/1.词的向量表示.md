# 1.one-hot encoding（传统方式）

- 定义：表示一个单词：只有一维是1 ，其他维是0。
- 缺点：语义鸿沟（同义词问题，one-hot没有语义）、维度灾难、稀疏、无法表示未出现在词表中的词。

# 2.count-based

## 2.1 基于词频统计

one-hot的增强版，引入了词频。

## 2.1 tf*idf

- **定义：**引入了逆文档频率。
  - **tf（词频）：**一词语出现的次数除以该文档的总词语数。
  - **idf（逆文档频率）：**文档频率。一词语出现在多少文档的数量除以总文档数。
- **假设**：如果某个词在一篇文章中出现的频率高，并且在其他文章中很少出，那么它很可能就反映了这篇文章的特性，因此要提高它的权值。
- ​

## 2.2 svd

## 2.3Glove 

# 3.分布式表示

## 3.1 原理

- **定义：**将one-hot压缩到低维空间，每一维可以看成词的语义或主题信息，语义相似的词语距离近。
- **优点：**维度压缩、语义提取解决语义鸿沟、基于学习模型可以对未出现在词表中的词进行表示。
- **方法：**LDA、Deep Learning。
- **核心假设：**具有相似上下文信息的词应该具有相似的词表示。
- **词关系：**Paradigmatic（同义词）和Syntagmatic（搭配出现）
- Distributional Representation VS Distributed Representation：
  - Distributional Representation是从分布式假设（由Harris在1954年提出，出现在相同上下文的词语语义相似）的角度，是一类获取词表示的方法。
  - 而Distributed Representation指的是文本表示的形式，就是低维、稠密的连续向量

## 3.2 传统方法——语言模型

## 3.3 深度学习方法——NNLM

## 3.4 深度学习方法——CBOW/Skipgram

- NNLM方法的升级版。
- 去除隐藏层。
- 去不考虑词序：汉字顺序并不影响阅读。

### 3.4.1 CBOW

- BOW的升级版。

### 3.4.2 skip-gram

## 3.5 实现工具

- word2vec
- gensim
- fasttext